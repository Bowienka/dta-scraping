import streamlit as st
import requests
from bs4 import BeautifulSoup
import time

st.title("Cvi캜en칤 - Web Scraping")


# Spole캜n칠 funkce
def get_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }


# Cvi캜en칤 1: Sta쬰n칤 a extrakce titulku
st.header("Cvi캜en칤 1: Extrakce titulku str치nky")

with st.expander("Zobrazit zad치n칤"):
    st.markdown("""
    **칔kol**: Napi코te program, kter칳 st치hne webovou str치nku a vyp칤코e jej칤 titulek (`<title>` tag).

    **C칤l**: Nau캜it se z치kladn칤 pr치ci s requests a BeautifulSoup pro extrakci jednoduch칳ch dat.
    """)

with st.expander("Zobrazit n치pov캩du"):
    st.markdown("""
    1. Pou쬴jte `requests.get()` pro sta쬰n칤 str치nky
    2. Vytvo콏te BeautifulSoup objekt z `response.text`
    3. Najd캩te element title pomoc칤 `soup.title`
    """)

url_title = st.text_input(
    "Zadejte URL adresu pro extrakci titulku:",
    "https://example.com"
)

if st.button("Z칤skat titulek", key="btn_title"):
    with st.spinner("Stahuji str치nku..."):
        try:
            response = requests.get(url_title, headers=get_headers(), timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            if soup.title:
                st.success(f"Titulek str치nky: {soup.title.text.strip()}")
            else:
                st.warning("Str치nka nem치 titulek")

            with st.expander("Zobrazit zdrojov칳 k칩d"):
                st.code("""
def get_page_title(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    return soup.title.text.strip() if soup.title else None
""", language="python")

        except Exception as e:
            st.error(f"Chyba p콏i stahov치n칤: {str(e)}")

# Cvi캜en칤 2: Extrakce v코ech odkaz콢
st.header("Cvi캜en칤 2: Extrakce odkaz콢")

with st.expander("Zobrazit zad치n칤"):
    st.markdown("""
    **칔kol**: Vytvo콏te program, kter칳 st치hne str치nku a vyp칤코e seznam v코ech unik치tn칤ch odkaz콢.

    **C칤l**: Nau캜it se pracovat s HTML atributy a zpracov치vat v칤ce element콢 najednou.
    """)

with st.expander("Zobrazit n치pov캩du"):
    st.markdown("""
    1. Pou쬴jte `find_all('a')` pro nalezen칤 v코ech odkaz콢
    2. Pro ka쬯칳 odkaz z칤skejte atribut 'href' pomoc칤 `get()`
    3. Pou쬴jte `set()` pro odstran캩n칤 duplicit
    4. Filtrujte neplatn칠 odkazy (None, pr치zdn칠, #)
    """)

url_links = st.text_input(
    "Zadejte URL adresu pro extrakci odkaz콢:",
    "https://example.com"
)

if st.button("Z칤skat odkazy", key="btn_links"):
    with st.spinner("Stahuji str치nku..."):
        try:
            response = requests.get(url_links, headers=get_headers(), timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            links = set()
            for a in soup.find_all('a'):
                href = a.get('href')
                if href and not href.startswith('#'):
                    links.add(href)

            if links:
                st.success(f"Nalezeno {len(links)} unik치tn칤ch odkaz콢:")
                for link in sorted(links):
                    st.write(f"- {link}")
            else:
                st.warning("Nebyly nalezeny 쮂멳n칠 odkazy")

            with st.expander("Zobrazit zdrojov칳 k칩d"):
                st.code("""
def get_all_links(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    links = set()
    for a in soup.find_all('a'):
        href = a.get('href')
        if href and not href.startswith('#'):
            links.add(href)

    return sorted(links)
""", language="python")

        except Exception as e:
            st.error(f"Chyba p콏i stahov치n칤: {str(e)}")

st.info("""
游눠 **Tipy pro scraping:**
1. V쬯y pou쮂셨ejte User-Agent hlavi캜ky
2. Implementujte timeouty
3. O코et콏ujte chyby
4. Respektujte robots.txt
5. P콏idejte zpo쬯캩n칤 mezi po쬬davky p콏i scrapov치n칤 v칤ce str치nek
""")